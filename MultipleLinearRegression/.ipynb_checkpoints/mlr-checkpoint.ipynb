{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cálculo1,LPT,P1,IC,Cálculo2,cra \n",
    "#names=[\"size\",\"bedroom\",\"price\"]\n",
    "rdata = pd.read_csv('sample_treino.csv') #read the data\n",
    "#rdata = pd.read_csv('home.txt',names=[\"size\",\"bedroom\",\"price\"])\n",
    "#rdata = (rdata - rdata.mean())/rdata.std()\n",
    "#\n",
    "data = rdata.iloc[:,0:len(rdata.columns)-1]\n",
    "#add extra columns with ones\n",
    "data = np.c_[np.ones(len(rdata)),data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.iloc[:,2:3].values # y=.values converts it from pandas.core.frame.DataFrame to numpy.ndarray\n",
    "y = rdata.iloc[:,len(rdata.columns)-1:len(rdata.columns)].values\n",
    "theta = np.zeros([1,len(rdata.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ RSS(w) = \\frac{1}{N} \\sum_{i=1}^{N} {(X^T \\cdot W - Y_i)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X,Y,w):\n",
    "    return np.sum(np.power(((X @ w.T)-Y),2))/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.47995385562645\n"
     ]
    }
   ],
   "source": [
    "print(MSE(data,y,theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ w^{t+1} = w^t -2H^t (y-H_w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(w,X,Y,learningRate):\n",
    "    #return w - ((learningRate/len(X)) * np.sum(X * (X @ w.T - Y), axis=0) )\n",
    "    return w - ((learningRate) * np.sum(X * (X @ w.T - Y), axis=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-4d836a058ccf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mgradient_descent_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.04\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-4d836a058ccf>\u001b[0m in \u001b[0;36mgradient_descent_runner\u001b[1;34m(starting_w, X, Y, learning_rate, epsilon)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print('{:.3f}'.format(np.linalg.norm(grad)),np.linalg.norm(grad)>=epsilon)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#print(np.linalg.norm(grad))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def gradient_descent_runner(starting_w,X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    w_old = [np.inf]\n",
    "    grad = np.array([np.inf])\n",
    "    i = 0\n",
    "    #while ( (np.linalg.norm(grad)>=epsilon) and (np.linalg.norm(grad) <= np.linalg.norm(np.array(w_old))) ):\n",
    "    while (np.linalg.norm(grad)>=epsilon):\n",
    "        w_old = w\n",
    "        w = step_gradient(w, X, Y, learning_rate)\n",
    "        #print('{:.3f}'.format(np.linalg.norm(grad)),np.linalg.norm(grad)>=epsilon)\n",
    "        grad = np.array(w)\n",
    "        #print(np.linalg.norm(grad))\n",
    "        i+= 1\n",
    "    print(np.linalg.norm(grad))\n",
    "    return w,i\n",
    "\n",
    "gradient_descent_runner(theta,data,y,0.01,0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner(starting_w,X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    w_old = [np.inf]\n",
    "    grad = np.array([np.inf])\n",
    "    #print(np.linalg.norm(grad))\n",
    "    #print(np.linalg.norm(np.array(w_old)))\n",
    "    i = 0\n",
    "    while (i < 1000):\n",
    "    #while (np.linalg.norm(grad) <= np.linalg.norm(np.array(w_old))):\n",
    "        w_old = w\n",
    "        w = step_gradient(w, X, Y, learning_rate)\n",
    "        #print('{:.3f}'.format(np.linalg.norm(grad)),np.linalg.norm(grad)>=epsilon)\n",
    "        grad = np.array(w)\n",
    "        if (i % 100 == 0):\n",
    "            print(np.linalg.norm(grad))\n",
    "        i+= 1\n",
    "    #print(\"sss\",np.linalg.norm(grad))\n",
    "    return w,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4427959807964417\n",
      "0.8863627022952231\n",
      "0.8863627023548142\n",
      "0.8863627023548142\n",
      "0.8863627023548142\n",
      "0.8863627023548142\n",
      "0.8863627023548142\n",
      "0.8863627023548142\n",
      "0.8863627023548142\n",
      "0.8863627023548142\n",
      "0.8863627023548142\n",
      "[[-1.10467191e-16  8.84765988e-01 -5.31788197e-02]] 1000\n",
      "0.26137296107808394\n"
     ]
    }
   ],
   "source": [
    "ws,i = gradient_descent_runner(theta,data,y,0.01,0.879)\n",
    "print(np.linalg.norm(np.array(ws)))\n",
    "print(ws,i)\n",
    "print(MSE(data, y,ws))\n",
    "#[[-1.10868761e-16  8.78503652e-01 -4.69166570e-02]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import linear_model\n",
    "#lm = linear_model.LinearRegression()\n",
    "#model = lm.fit(X,y)\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4238032409512109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.10304143, 0.0464367 , 0.16409834, 0.38117843,\n",
       "        0.02027816]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression  \n",
    "regressor = LinearRegression()  \n",
    "#regressor.fit(X_train, y_train)\n",
    "regressor.fit(data, y)\n",
    "print(regressor.score(data, y))\n",
    "regressor.coef_\n",
    "#[rdata.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coeff_df = pd.DataFrame(regressor.coef_, rdata.columns[:-1], columns=['Coefficient'])  \n",
    "#coeff_df  \n",
    "#y_pred = regressor.predict(X_test)\n",
    "y_pred = regressor.predict(data)\n",
    "#df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})  \n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.4898513745150918\n",
      "Mean Squared Error: 0.41133758922770963\n",
      "Root Mean Squared Error: 0.6413560549552094\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [18, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-a03ae584727b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean Absolute Error:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean Squared Error:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Root Mean Squared Error:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \"\"\"\n\u001b[0;32m    169\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 170\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    171\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n\u001b[0;32m    172\u001b[0m                                weights=sample_weight, axis=0)\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \"\"\"\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [18, 10]"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "def gradientDescent(X,y,theta,iters,alpha):\n",
    "    mse = np.zeros(iters)\n",
    "    for i in range(iters):\n",
    "        \n",
    "        msg[i] = MSE_ok(X, y, theta)\n",
    "    \n",
    "    return theta,cost\n",
    "\n",
    "#running the gd and cost function\n",
    "g,cost = gradientDescent(X,y,theta,iters,alpha)\n",
    "print(g)\n",
    "\n",
    "finalCost = computeCost(X,y,g)\n",
    "print(finalCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    res = Y - np.dot(X,w_current)\n",
    "    b_gradient = np.sum(res)\n",
    "    X = X[:,1][:,np.newaxis]\n",
    "    m_gradient = np.sum(np.multiply(res,X))\n",
    "    new_w = np.array([(w_current[0] + (2 * learningRate * b_gradient)),\n",
    "             (w_current[1] + (2 * learningRate * m_gradient))])\n",
    "    return [new_w,b_gradient,m_gradient]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(\\hat{w})=\\frac{1}{N}(y-\\hat{\\mathbf{w}}^T\\mathbf{x})^T(y-\\hat{\\mathbf{w}}^T\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    res = Y - np.dot(X,w_current)\n",
    "    b_gradient = np.sum(res)\n",
    "    X = X[:,1][:,np.newaxis]\n",
    "    m_gradient = np.sum(np.multiply(res,X))\n",
    "    new_w = np.array([(w_current[0] + (2 * learningRate * b_gradient)),\n",
    "             (w_current[1] + (2 * learningRate * m_gradient))])\n",
    "    return [new_w,b_gradient,m_gradient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf,np.inf])\n",
    "    i = 0\n",
    "    while (np.linalg.norm(grad)>=epsilon):\n",
    "        w,b_gradient,m_gradient = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        grad = np.array([b_gradient,m_gradient])\n",
    "        #print(np.linalg.norm(grad))\n",
    "        if i % 1000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.genfromtxt(\"income.csv\", delimiter=\",\")\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "X = points[:,[0,1]]\n",
    "Y = points[:,2][:,np.newaxis]\n",
    "init_w = np.zeros((2,1))\n",
    "learning_rate = 0.0001\n",
    "#num_iterations = 10000\n",
    "epsilon = 0.5\n",
    "print(\"Starting gradient descent at w0 = {0}, w1 = {1}, error = {2}\".format(init_w[0], init_w[1], compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, error = {2}\".format(w[0], w[1], compute_mse_vectorized(w,X,Y)))\n",
    "print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
